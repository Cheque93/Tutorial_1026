{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zVdi3nsFfbk6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torchvision import transforms\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "import natsort\n",
        "from sklearn.metrics import mean_squared_error \n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ugBTSaPAfp4a"
      },
      "outputs": [],
      "source": [
        "# 배치 크기\n",
        "batch_size = 4\n",
        "\n",
        "# 이미지의 채널 수로, GRAY 이미지이기 때문에 1 로 설정합니다.\n",
        "nc = 1\n",
        "\n",
        "# 생성자를 통과하는 특징 데이터들의 채널 크기\n",
        "ngf = 64\n",
        "\n",
        "# 구분자를 통과하는 특징 데이터들의 채널 크기\n",
        "ndf = 256\n",
        "\n",
        "# 학습할 에폭 수\n",
        "num_epochs = 20\n",
        "\n",
        "# 옵티마이저의 학습률\n",
        "lr_D = 0.0005\n",
        "lr_G = 0.001\n",
        "\n",
        "# Adam 옵티마이저의 beta 하이퍼파라미터\n",
        "beta = (0.9,0.999)\n",
        "\n",
        "# 학습에 사용될 장치를 선택합니다.\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "u_hl3Dl_ftsg"
      },
      "outputs": [],
      "source": [
        "class Custom_datasets(Dataset):\n",
        "    def __init__(self,img_path,gt_path,transform=None,img_size=(256,256)):\n",
        "        self.img = img_path\n",
        "        self.gt = gt_path\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.img[idx])\n",
        "        gt = Image.open(self.gt[idx])\n",
        "        randomSeed = random.randint(0, 1000)\n",
        "        if self.transform:\n",
        "            random.seed(randomSeed)\n",
        "            torch.manual_seed(randomSeed)\n",
        "            image = self.transform(image)\n",
        "            random.seed(randomSeed)\n",
        "            torch.manual_seed(randomSeed)\n",
        "            gt = self.transform(gt)\n",
        "\n",
        "        else:\n",
        "            return image\n",
        "\n",
        "        return [image, gt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m59PL9risTSF",
        "outputId": "e8a092ab-8033-4846-8e1f-4a63346352aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/MyDrive/Tutorial_1026\n",
            "fatal: destination path '.' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive/')\n",
        "path = '/content/drive/MyDrive/Tutorial_1026/'\n",
        "os.makedirs(path,exist_ok=True)\n",
        "os.chdir(path)\n",
        "print(os.getcwd())\n",
        "!git clone https://github.com/Cheque93/Tutorial_1026.git ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8OgKlYCwf4AO"
      },
      "outputs": [],
      "source": [
        "train_path = path + 'imgs/train/'\n",
        "train_noise = natsort.natsorted(np.char.add(train_path+'noise/',os.listdir(train_path+'noise')))\n",
        "train_gt = natsort.natsorted(np.char.add(train_path+'gt/',os.listdir(train_path+'gt')))\n",
        "\n",
        "valid_path = path + 'imgs/valid/'\n",
        "valid_noise = natsort.natsorted(np.char.add(valid_path+'noise/',os.listdir(valid_path+'noise')))\n",
        "valid_gt = natsort.natsorted(np.char.add(valid_path+'gt/',os.listdir(valid_path+'gt')))\n",
        "\n",
        "test_path = path + 'imgs/test/'\n",
        "test_noise = natsort.natsorted(np.char.add(test_path+'noise/',os.listdir(test_path+'noise')))\n",
        "test_gt = natsort.natsorted(np.char.add(test_path+'gt/',os.listdir(test_path+'gt')))\n",
        "\n",
        "data_transforms = transforms.Compose([  transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.5),(0.5))\n",
        "                                        ])\n",
        "train_set = Custom_datasets(train_noise,train_gt,data_transforms)\n",
        "valid_set = Custom_datasets(valid_noise,valid_gt,data_transforms)\n",
        "test_set = Custom_datasets(test_noise,test_gt,data_transforms)\n",
        "dataloaders = {\n",
        "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
        "    'valid': DataLoader(valid_set, batch_size=4, shuffle=True, num_workers=0),\n",
        "    'test': DataLoader(test_set, batch_size=1, shuffle=False, num_workers=0)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rbcV844lgicF"
      },
      "outputs": [],
      "source": [
        "# netG와 netD에 적용시킬 커스텀 가중치 초기화 함수\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv2d') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZhZqGdbfglzz"
      },
      "outputs": [],
      "source": [
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1,stride=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.PReLU()\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "    \n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.PReLU()\n",
        "            )\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "    \n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.inc = nn.Conv2d(n_channels, 64,kernel_size=3, padding=1)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.up1 = Up(256, 128)\n",
        "        self.up2 = Up(128, 64)\n",
        "        self.out_conv = nn.Conv2d(64, n_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x = self.up1(x3, x2)\n",
        "        x = self.up2(x, x1)\n",
        "        logits =  self.out_conv(x)\n",
        "        \n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "d7raek2Ngpu8"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self,n_channels):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (nc) x 64 x 64\n",
        "            nn.Conv2d(n_channels, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf) x 32 x 32\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*2) x 16 x 16\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*4) x 8 x 8\n",
        "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.main(input)\n",
        "        x=  F.avg_pool2d(x, x.size()[2:]).view(x.size()[0], -1)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNItdMSfgqQB",
        "outputId": "3ebbbcd2-2452-44b7-c973-d2649332cee1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (main): Sequential(\n",
              "    (0): Conv2d(1, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (2): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (5): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (6): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (8): Conv2d(1024, 2048, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (9): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
              "    (11): Conv2d(2048, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "netG = UNet(nc).to(device)\n",
        "netD = Discriminator(nc).to(device)\n",
        "netG.apply(weights_init)\n",
        "netD.apply(weights_init)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "98V0Okfxgv4k"
      },
      "outputs": [],
      "source": [
        "# BCELoss 함수의 인스턴스를 생성합니다\n",
        "criterion_BCE = nn.BCELoss()\n",
        "criterion_MSE = nn.MSELoss()\n",
        "# 생성자의 학습상태를 확인할 잠재 공간 벡터를 생성합니다\n",
        "\n",
        "# 학습에 사용되는 참/거짓의 라벨을 정합니다\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "\n",
        "# G와 D에서 사용할 Adam옵티마이저를 생성합니다\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr_D, betas=beta)\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr_G, betas=beta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwfahujRgx3C",
        "outputId": "eaf29c19-0df0-47d6-8a57-838584a0e530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training Loop...\n",
            "train: [1/20][25/25]\tLoss_D: 5.6744\tLoss_G: 0.1282\tD(x): 0.0900\tD(G(z)): 0.0579 / 0.0753\n"
          ]
        }
      ],
      "source": [
        "G_losses = []\n",
        "D_losses = []\n",
        "val_G_losses = []\n",
        "val_D_losses = []\n",
        "best_loss = 1e5\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "# 에폭(epoch) 반복\n",
        "for epoch in range(1,num_epochs+1):\n",
        "    # 한 에폭 내 train 과 valid 페이즈를 나누어 \n",
        "    # train 페이즈에만 파라미터 업데이트하도록 설정합니다\n",
        "    tmp_val_G_loss = 0\n",
        "    tmp_val_D_loss = 0\n",
        "    tmp_train_G_loss = 0\n",
        "    tmp_train_D_loss = 0\n",
        "    for phase in ['train','valid']:\n",
        "        if phase=='train':\n",
        "            netD.train()\n",
        "            netG.train()\n",
        "        else:\n",
        "            netD.eval()\n",
        "            netG.eval()\n",
        "        for i, data in enumerate(dataloaders[phase],start=1):\n",
        "            ############################\n",
        "            # (1) D 신경망을 업데이트 합니다: log(D(x)) + log(1 - D(G(z)))를 최대화 합니다\n",
        "            ###########################\n",
        "            ## 진짜 데이터들로 학습을 합니다\n",
        "            train = data[0]\n",
        "            gt = data[1]\n",
        "            optimizerD.zero_grad()\n",
        "            # 배치들의 사이즈나 사용할 디바이스에 맞게 조정합니다\n",
        "            real_cpu = gt.to(device)\n",
        "            b_size = real_cpu.size(0)\n",
        "            label = torch.full((b_size,), real_label,\n",
        "                               dtype=torch.float, device=device)\n",
        "            # 진짜 데이터들로 이루어진 배치를 D에 통과시킵니다\n",
        "            output = netD(real_cpu).view(-1)\n",
        "            # 손실값을 구합니다\n",
        "            errD_real = criterion_BCE(output, label)\n",
        "            # 역전파의 과정에서 변화도를 계산합니다\n",
        "            if phase == 'train':\n",
        "                errD_real.backward()\n",
        "                \n",
        "            D_x = output.mean().item()\n",
        "           \n",
        "            ## 가짜 데이터들로 학습을 합니다\n",
        "            # 생성자에 사용할 노이즈 이미지를 불러옵니다.\n",
        "            noise = train.to(device)\n",
        "            # G를 이용해 노이즈가 제거된 이미지를 생성합니다\n",
        "            fake = netG(noise)\n",
        "            label.fill_(fake_label)\n",
        "            # D를 이용해 데이터의 진위를 판별합니다\n",
        "            output = netD(fake.detach()).view(-1)\n",
        "            # D의 손실값을 계산합니다\n",
        "            errD_fake = criterion_BCE(output, label)\n",
        "            # 역전파를 통해 변화도를 계산합니다. 이때 앞서 구한 변화도에 더합니다(accumulate)\n",
        "            \n",
        "            D_G_z1 = output.mean().item()\n",
        "            # 가짜 이미지와 진짜 이미지 모두에서 구한 손실값들을 더합니다\n",
        "            # 이때 errD는 역전파에서 사용되지 않고, 이후 학습 상태를 리포팅(reporting)할 때 사용합니다\n",
        "            errD = errD_real + errD_fake\n",
        "            # D를 업데이트 합니다\n",
        "            if phase == 'train':\n",
        "                errD_fake.backward()\n",
        "                optimizerD.step()\n",
        "    \n",
        "            ############################\n",
        "            # (2) G 신경망을 업데이트 합니다: log(D(G(z)))를 최대화 합니다\n",
        "            ###########################\n",
        "            optimizerG.zero_grad()\n",
        "            label.fill_(real_label)  # 생성자의 손실값을 구하기 위해 진짜 라벨을 이용할 겁니다\n",
        "            # 우리는 방금 D를 업데이트했기 때문에, D에 다시 가짜 데이터를 통과시킵니다.\n",
        "            # 이때 G는 업데이트되지 않았지만, D가 업데이트 되었기 때문에 앞선 손실값가 다른 값이 나오게 됩니다\n",
        "            output = netD(fake).view(-1)\n",
        "            # G의 손실값을 구합니다\n",
        "            # BCE_loss = criterion_BCE(output, label)\n",
        "            MSE_loss = criterion_MSE(fake, real_cpu)\n",
        "            errG =   torch.sqrt(MSE_loss)\n",
        "            # G의 변화도를 계산합니다\n",
        "            D_G_z2 = output.mean().item()\n",
        "            # G를 업데이트 합니다\n",
        "            if phase == 'train':\n",
        "                errG.backward()\n",
        "                optimizerG.step()\n",
        "                tmp_train_G_loss += errG.item()\n",
        "                tmp_train_D_loss += errD.item()\n",
        "            else:\n",
        "                tmp_val_G_loss += errG.item()\n",
        "                tmp_val_D_loss += errD.item()\n",
        "            # 훈련 상태를 출력합니다\n",
        "            if phase=='train':\n",
        "                print('\\rtrain: [%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                      % (epoch, num_epochs, i, len(dataloaders['train']),\n",
        "                          errD.item(), errG.item(), D_x, D_G_z1, D_G_z2),end='')\n",
        "            else:\n",
        "                print('\\rvalid: [%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                      % (epoch, num_epochs, i, len(dataloaders['valid']),\n",
        "                          errD.item(), errG.item(), D_x, D_G_z1, D_G_z2),end='')\n",
        "        if phase == 'train':\n",
        "          G_losses.append(tmp_train_G_loss/len(dataloaders['train']))\n",
        "          D_losses.append(tmp_train_D_loss/len(dataloaders['train']))\n",
        "        else:\n",
        "          val_G_losses.append(tmp_val_G_loss/len(dataloaders['valid']))\n",
        "          val_D_losses.append(tmp_val_D_loss/len(dataloaders['valid']))\n",
        "          if val_G_losses[-1]<best_loss:\n",
        "            best_loss = val_G_losses[-1]\n",
        "            print(\"\\nsaving best model with %.4f loss\"%(best_loss),end='')\n",
        "            best_modelG_wts = copy.deepcopy(netG.state_dict())\n",
        "            best_modelD_wts = copy.deepcopy(netD.state_dict())\n",
        "      \n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qp2fh2lOdi5d"
      },
      "outputs": [],
      "source": [
        "netG.load_state_dict(best_modelG_wts)\n",
        "netD.load_state_dict(best_modelD_wts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vF43kMkhgzUZ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Generator Loss with Training and Validation\")\n",
        "plt.plot(G_losses,label=\"train_G\")\n",
        "plt.plot(val_G_losses,label=\"valid_G\")\n",
        "plt.xlabel(\"epoches\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Discriminator Loss with Training and Validation\")\n",
        "plt.plot(D_losses,label=\"train_D\")\n",
        "plt.plot(val_D_losses,label=\"valid_D\")\n",
        "plt.xlabel(\"Epoches\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s_fMWbMkH9-"
      },
      "outputs": [],
      "source": [
        "# test셋에서 검증합니다.\n",
        "netG.eval()\n",
        "tb = iter(dataloaders['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kS5ybzskKOG"
      },
      "outputs": [],
      "source": [
        "# test셋에서 데이터를 불러옵니다.\n",
        "test_batch = next(tb)\n",
        "t_noise = test_batch[0].to(device)\n",
        "t_img = test_batch[0].cpu().detach().numpy()[0].transpose((1,2,0))\n",
        "t_gt = test_batch[1].cpu().detach().numpy()[0].transpose((1,2,0))\n",
        "# 학습된 모델로 noise 이미지를 denoising 합니다.\n",
        "test_result = netG(t_noise)\n",
        "test_result = test_result.cpu().detach().numpy()[0].transpose((1,2,0))\n",
        "# 정규화\n",
        "test_result = (test_result+1)*0.5\n",
        "t_gt = (t_gt+1)*0.5\n",
        "t_img = (t_img+1)*0.5\n",
        "# 결과를 출력합니다.\n",
        "plt.imshow(test_result[:,:,0],cmap='gray')\n",
        "plt.show()\n",
        "plt.imshow(t_gt[:,:,0],cmap='gray')\n",
        "plt.show()\n",
        "# mean squared error를 계산합니다.\n",
        "mse = mean_squared_error(t_gt[:,:,0],test_result[:,:,0])\n",
        "print(mse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTKbq9s2Z1n-"
      },
      "outputs": [],
      "source": [
        "# psnr을 계산하기 위한 함수\n",
        "def calc_psnr(truth,compare,pixel_max=255.0):\n",
        "    mse = np.mean((truth-compare)**2)\n",
        "    if mse == 0:\n",
        "        return 100\n",
        "    return 20*math.log10(pixel_max/math.sqrt(mse))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPPyfPSGcgdX"
      },
      "outputs": [],
      "source": [
        "# 전체 test셋에 대해 pnsr과 mse를 계산합니다.\n",
        "tb = iter(dataloaders['test'])\n",
        "total_mse = []\n",
        "total_psnr = []\n",
        "for test_batch in tb:\n",
        "    t_noise = test_batch[0].cuda()\n",
        "    t_img = test_batch[0].cpu().detach().numpy()[0].transpose((1,2,0))\n",
        "    t_gt = test_batch[1].cpu().detach().numpy()[0].transpose((1,2,0))\n",
        "    test_result = netG(t_noise).cpu().detach().numpy()[0]\n",
        "    test_result = test_result.transpose((1,2,0))\n",
        "    test_result = (test_result+1)*127.5\n",
        "    t_gt = (t_gt+1)*127.5\n",
        "    t_img = (t_img+1)*127.5\n",
        "    mse = mean_squared_error(t_gt[:,:,0]/255,test_result[:,:,0]/255)\n",
        "    #print(mse)\n",
        "    p = calc_psnr(t_gt[:,:,0],test_result[:,:,0])\n",
        "    #print(p)\n",
        "    total_mse.append(mse)\n",
        "    total_psnr.append(p)\n",
        "# 평균 mse와 pnsr을 계산합니다.\n",
        "print(np.mean(total_mse))\n",
        "print(np.mean(total_psnr))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHEQCVI7dnrt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}